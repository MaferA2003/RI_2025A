# -*- coding: utf-8 -*-
"""13langchain.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HC41A8Rb0hhfjMJ_Xl3Ej9Ah6NpkxnjB

# Ejercicio 13: LangChain

LangChain es un _framework_ de código abierto diseñado para facilitar el desarrollo de aplicaciones que combinan modelos de lenguaje LLMs con datos, herramientas externas y memoria. Está especialmente pensado para construir aplicaciones complejas basadas en IA, como sistemas _Retrieval-Augmented Generation_, asistentes conversacionales inteligentes, agentes autónomos y sistemas con razonamiento compuesto.

## Parte 1: Carga y preprocesamiento del corpus
"""

!pip install python-dotenv

!pip install python-dotenv kaggle

import os

path = "../data/13langchain"
os.makedirs(path, exist_ok=True)

from google.colab import files

uploaded = files.upload()  # Selecciona tu archivo `.env`

from dotenv import load_dotenv
load_dotenv("/content/.env")

import os

os.environ["KAGGLE_USERNAME"] = os.getenv("KAGGLE_USERNAME")
os.environ["KAGGLE_KEY"] = os.getenv("KAGGLE_KEY")

import json
import os

# Crear contenido del archivo JSON
kaggle_json = {
    "username": os.environ["KAGGLE_USERNAME"],
    "key": os.environ["KAGGLE_KEY"]
}

# Guardar en ~/.kaggle
os.makedirs("/root/.kaggle", exist_ok=True)
with open("/root/.kaggle/kaggle.json", "w") as f:
    json.dump(kaggle_json, f)

# También guardar en ~/.config/kaggle
os.makedirs("/root/.config/kaggle", exist_ok=True)
with open("/root/.config/kaggle/kaggle.json", "w") as f:
    json.dump(kaggle_json, f)

# Establecer permisos
os.chmod("/root/.kaggle/kaggle.json", 600)
os.chmod("/root/.config/kaggle/kaggle.json", 600)

import kaggle

dataset = "rajneesh231/lex-fridman-podcast-transcript"
path = "/content/13langchain"
os.makedirs(path, exist_ok=True)

kaggle.api.dataset_download_files(dataset, path=path, unzip=True)

print("✅ Dataset descargado en:", path)

import os
import pandas as pd

# Ruta donde se descargó el dataset
path = "/content/13langchain"

# Verifica el nombre exacto del archivo CSV descargado
# (en este caso: podcastdata_dataset.csv, pero puede variar)
file_path = os.path.join(path, "podcastdata_dataset.csv")

# Cargar el archivo CSV en un DataFrame
df = pd.read_csv(file_path)

# Mostrar las primeras filas
df.head()

from langchain.schema import Document

# Convertir cada fila en un Document
documents = [
    Document(
        page_content=row["text"],
        metadata={"id": row["id"], "guest": row["guest"], "title": row["title"]},
    )
    for _, row in df.iterrows()
]

"""## Parte 2: Segmentación y embeddings"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = splitter.split_documents(documents)

!pip install -U langchain-community

from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

texts = [chunk.page_content for chunk in chunks]
vectorstore = FAISS.from_texts(texts, embeddings)

vectorstore.save_local("index_13langchain")

"""## Parte 3: Indexación en FAISS"""

vectorstore = FAISS.from_texts(texts, embeddings)
vectorstore.save_local("index_13langchain_02")

import google.generativeai as genai
genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
for m in genai.list_models():
    print(m.name, m.supported_generation_methods)

"""## Parte 4: Creación de la cadena de recuperación"""

from langchain.chains import RetrievalQA
from langchain_google_genai import ChatGoogleGenerativeAI

# Inicializar Gemini LLM
llm = ChatGoogleGenerativeAI(model="models/gemini-1.5-flash", temperature=0)

# Crear retriever desde tu índice vectorial (FAISS o Chroma)
retriever = vectorstore.as_retriever()

# Cadena de pregunta-respuesta
qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)

# Ejecutar una pregunta
response = qa_chain.run("What is AGI Artificial General Intelligence?")
print(response)

from langchain.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Usa solo el siguiente contexto para responder a la pregunta.
Si la respuesta no está explícita en el contexto, responde exactamente:
"No encontré información suficiente en el corpus."

Contexto:
{context}

Pregunta: {question}
Respuesta:
"""
)

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt}
)

result = qa_chain.invoke({"query": "¿Qué es AGI Artificial General Intelligence?"})
print(result["result"])

qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=retriever,
    return_source_documents=True
)

result = qa_chain.invoke({"query": "¿Qué es AGI Artificial General Intelligence?"})
print(result["result"])
print(result["source_documents"])

