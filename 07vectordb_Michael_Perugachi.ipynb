{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcf314626bfa1d6e",
   "metadata": {
    "id": "bcf314626bfa1d6e"
   },
   "source": [
    "# Ejercicio 7: Bases de Datos Vectoriales\n",
    "\n",
    "## Michael Perugachi\n",
    "\n",
    "## Objetivo de la pr\u00e1ctica\n",
    "\n",
    "Entender el concepto de Bases de Datos Vectoriales y saber utilizar las herramientas actuales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6969f64a18e27b98",
   "metadata": {
    "id": "6969f64a18e27b98"
   },
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "\n",
    "Vamos a utilizar la API de Kaggle para acceder al dataset _Wikipedia Text Corpus for NLP and LLM Projects_\n",
    "\n",
    "El corpus est\u00e1 disponible desde este [link](https://www.kaggle.com/datasets/gzdekzlkaya/wikipedia-text-corpus-for-nlp-and-llm-projects?utm_source=chatgpt.com)\n",
    "\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759acb68ad40d112",
   "metadata": {
    "id": "759acb68ad40d112"
   },
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from kagglehub import KaggleDatasetAdapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5141c83c549f5e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "ea5141c83c549f5e",
    "outputId": "d1ff7dd3-798c-4b39-c7bb-ca99368efe2d"
   },
   "outputs": [],
   "source": [
    "# Set the path to the file you'd like to load\n",
    "file_path = \"wikipedia_text_corpus.csv\"\n",
    "\n",
    "# Load the latest version\n",
    "df = kagglehub.dataset_load(\n",
    "  KaggleDatasetAdapter.PANDAS,\n",
    "  \"gzdekzlkaya/wikipedia-text-corpus-for-nlp-and-llm-projects\",\n",
    "  file_path,\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000da740833ca58",
   "metadata": {
    "id": "a000da740833ca58"
   },
   "source": [
    "## Parte 1: Generaci\u00f3n de Embeddings\n",
    "\n",
    "Vamos a utilizar E5 como modelo de embeddings.\n",
    "\n",
    "La documentaci\u00f3n de E5 est\u00e1 disponible desde este [link](https://huggingface.co/intfloat/e5-base-v2)\n",
    "\n",
    "### Actividad\n",
    "\n",
    "1. Normalizar el corpus\n",
    "2. Definir una funci\u00f3n `chunk_text`, y dividir los textos en _chunks_.\n",
    "3. Generar embeddings por cada _chunk_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639f5ff5b295666d",
   "metadata": {
    "id": "639f5ff5b295666d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "outputId": "f6e11c79-fb10-4f65-9fbc-0480439dfdaf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import re\n",
    "\n",
    "df = df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "\n",
    "# Limpieza b\u00e1sica\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].astype(str).map(normalize_text)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c97a94faca1c4a1",
   "metadata": {
    "id": "9c97a94faca1c4a1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a25c00e5-dc9a-4425-c269-3b7f962e6e58"
   },
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_chars: int = 800, overlap: int = 100):\n",
    "    \"\"\"\n",
    "    Chunking por caracteres.\n",
    "    max_chars ~ 600-1000 suele funcionar bien.\n",
    "    overlap ayuda a no cortar ideas a la mitad.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "    while start < n:\n",
    "        end = min(start + max_chars, n)\n",
    "        chunk = text[start:end]\n",
    "        chunk = chunk.strip()\n",
    "        if len(chunk) > 0:\n",
    "            chunks.append(chunk)\n",
    "        if end == n:\n",
    "            break\n",
    "        start = max(0, end - overlap)\n",
    "    return chunks\n",
    "\n",
    "records = []\n",
    "for i, row in df.iterrows():\n",
    "    chunks = chunk_text(row[\"text_norm\"], max_chars=800, overlap=100)\n",
    "    for j, ch in enumerate(chunks):\n",
    "        records.append({\n",
    "            \"doc_id\": int(i),\n",
    "            \"chunk_id\": j,\n",
    "            \"text\": ch\n",
    "        })\n",
    "\n",
    "chunks_df = pd.DataFrame(records)\n",
    "chunks_df.head(), len(chunks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8183ec2c4767e",
   "metadata": {
    "id": "69a8183ec2c4767e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461,
     "referenced_widgets": [
      "330ba17f40434ae1956f0aee63716806",
      "858f1b3766914e5f96f135104004114e",
      "5094806f00374929931b3fc57724f303",
      "47f0ca41a79e4fb487dda85f6b4ab75f",
      "6d474aa276ac476a98e54fe73c74a8e9",
      "6b51d374a5334fb684e84e3637653f12",
      "47ee4a1c5957402abe5cfe4b2412767f",
      "2d6d1136c1514bf7a2e049f5b055a201",
      "43ee5e42542f4c35a76ca28a0edcb6c7",
      "ce6c0b9d88694387807b6d80dc5302e8",
      "5a2e632aed1c4d60a7b2e48287d47886",
      "eebaeadc4ffe499e97bebcdf43a282a1",
      "6a548c4cf5904f50b5c7ed0bcfcbcb40",
      "25b433e561d745e1aa6d512d58b7974e",
      "679087d0d3f7458fbf9fb0e4507e1886",
      "985cad87bac84eec8bd4ee32622dd0bd",
      "bbdff408ae9b497986496c69ad55c0f1",
      "75e20195f14240498d984ed9ff1e9ff1",
      "80b48380063c4c4a88a0bd3a62b84ce3",
      "f9b5bbb0430146a598e8f87e7f161bbf",
      "26339634bd6b4baabbf4cb106ca4841d",
      "9f89d7e1e2104311abe5c98eca16effd",
      "07379e8b3eb340ddaa19e0e7f85195ea",
      "49a389ddd1aa48028472e7efe716053e",
      "255411e84bf7413380fa1a24d58f5952",
      "3770fac5fb154f62af140edc5e91528d",
      "e68c388c27a04e099c08fef87a7f447b",
      "f676618a947d485d83028109bdb5b54a",
      "ff0e6bf3fbe14ca4b3cb36aea547f31a",
      "cf4641bacc4d44eb88a5cce96415a6c8",
      "d02d6186e11b4e8c99c5964e22ea8a80",
      "7c88bc3f731b4a08999bd5bba904ade5",
      "e5751b052b5e4735a2551ffa7dba4daf",
      "2fa429d013e8430ab84222ffebb06cb2",
      "4be961f19fee4d549982da73ad8e601e",
      "f0612e99bfcc4f24aeef7d1d9fa78617",
      "0b9e0972a5e54fe2a80a4a2e5edda2e2",
      "582aa130198645f1b11498525e58cd56",
      "3cdf6880177f49e99e13a565c6f0a411",
      "2bea04ae30a94d5cbf98af64a1a58607",
      "7ade873d7f9340538dc3a929a7f1dce9",
      "5fe9963df5bd46a6b94f3509d939dd40",
      "887b3235e8ae420684a7480d4598de2a",
      "16637ca464da4b68ba9539e49d669f53",
      "7763d225d1d140afbcbf4bb2336372f0",
      "3e27a02746e840fb8033b550f8241257",
      "4039f73620f44cc8a4d7fbc8d752707b",
      "22f0f5d3b0c84420a55e6102b305a567",
      "560b138126914b77af2a720317dca2ba",
      "2c5ac14f1dac474c94704d4d08e452b8",
      "006521208a49438eaf7c438029c2e851",
      "fea82b0ca19c43aa98744b3c17753881",
      "852003da5d814de8af4768319e0842e4",
      "f92937a20eb943168e6fe32875d69cfc",
      "66a9feed0391466d95c538484583580b",
      "5492b565cb544833a57827b379a2cb09",
      "2bef131e7af848709a88d383dd948489",
      "49f9f4a3342b4648909830b9e4744062",
      "b98368da04c24bfbaf4239d1475d40d6",
      "2749e386d14640daad7f457cfabd4304",
      "7aa328ae2af34c8bb16d67b53e2ece46",
      "a7d12b3e7e694375b51782d2f2a2ce28",
      "ff112cc0bd8c451eba96f024965a72bb",
      "882b2b0310af46898f4fa6fc2b34976b",
      "502c8cdf568848df8f5d1537eddeefe6",
      "c1728a6148914e329b2b68c583554c38",
      "2d697f346c81408f8315cfcbe52ffcf7",
      "45cd6a2f07f6437ea3b9eb630b79a842",
      "5bd3703511d5427f9efd3ac36aefbfbe",
      "df6d1d9645ea45b3a6bce7a9c3781231",
      "99d598024e63458d9fe6887e069c4f76",
      "7047cbab568b4bf583cb01f8304fed40",
      "c4db873ec4304ec990bf53252e48005b",
      "2126a351539f44c59b91d1e8c8d2e6aa",
      "1e67b20f9cb947ce9971a4785ef0d888",
      "f74787beab0a4646a4e238293a92522f",
      "44cb7420ef574f84abebb0dde6a7a134",
      "e8c8c1dab06e4cb28e34d6c4e061f05a",
      "077e5cab72584fabac45cc86f207a371",
      "d81a04ebe68d413eb838390d0cd239de",
      "34dbf2c865d24faebd61432a25357f87",
      "4f5a790e5d88407f918ed415f5302e3a",
      "0b45f03f4be84fb29d79cbe37ffe2bff",
      "d74e87525c1740e7a83f374660802218",
      "937e7d15cd4241269c6c4b49b357fff2",
      "21827b46755042ebbb4a4be570258feb",
      "c00d5fbde7cf46fda469cc71465019bf",
      "25e184b9ce824b34bf79ce0bfde1e47c",
      "9fcccd5281754f90a1e8da249c951013",
      "fdd560ad97514b4f934e52011a8ce0f0",
      "4ea4891c8b804047adf28c64910ef0f4",
      "4f0d5159a40e4aa1843ab19a9ed38f4f",
      "6d77f11f75c44131800e1364cb0ac498",
      "89aff47ff8ed4923a831064a881e84cf",
      "d110fe5f072c45e8b2e62461e3a85c20",
      "cb51ad2db78847f38f43ac92ab543623",
      "5fef952d7716425e866dbcd54af575ec",
      "8d0de5cde8db4370a7d010bab27142ca",
      "60625cc19bec4391828658cc052f5857",
      "10c1eff3033c45d8b5d003ec625e8583",
      "3f399ed4f47a42d28f5b3fb98d445710",
      "084598120e7443d19b72f4d0d6b4ab86",
      "f166245c688649ab9941e99b4e2f39ae",
      "3b1d26aefb594158b6d3282620be74cb",
      "6ac5b150b9774f74b2a94697f1c31747",
      "99a627dde1164c02a00bf6f531005333",
      "e84760b6a7a245f381995acf48d80f97",
      "c211554c1c6c41d0ab86179e1ef6c609",
      "4aeed01843f946c39ac6ada0103bf83a",
      "41870395a6d7479f9350120890e97e52"
     ]
    },
    "outputId": "e909f3b4-58bc-4c6a-dc4f-c8d078e5cdce"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"intfloat/e5-base-v2\"   # recomendado para retrieval\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "# Textos a indexar (pasajes)\n",
    "passages = [\"passage: \" + t for t in chunks_df[\"text\"].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6f9319fe0075fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b956fca6b8b14bf6a98f4d27caf0d54e",
      "d62fef2b238c48c284c3b99fa4356999",
      "e7c9b27326d74e1aadfc756eef1b967d",
      "6049fe146014403a8fb78149d5acbd05",
      "15578e9e6add486eaa824dd13f4143a2",
      "5c286de472604536afb3c9c638230d18",
      "fa5524e561d141ceb7d2def85181fd17",
      "db2d703e0b9249dc9d8975ff8caaeaca",
      "ce7e690811bf47e88cab5e76f7613005",
      "95d745feb9204b82ba0a2be70cdd58be",
      "f40baf29d7134256907d4ed908e01f03"
     ]
    },
    "id": "cc6f9319fe0075fa",
    "outputId": "f1f5c794-68fa-481e-8431-267d4a48bf87"
   },
   "outputs": [],
   "source": [
    "# Embeddings (N x D)\n",
    "# Se debe usar normalize_embeddings=True para similitud coseno\n",
    "embeddings = model.encode(\n",
    "    passages,\n",
    "    batch_size=16,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True,\n",
    "    normalize_embeddings=True\n",
    ").astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73da34fcbbdd0b9",
   "metadata": {
    "id": "f73da34fcbbdd0b9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ae880548-54d9-4cbb-ec87-d55ddd560c83"
   },
   "outputs": [],
   "source": [
    "print(embeddings.shape, embeddings.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1975f9c2556bb8e",
   "metadata": {
    "id": "1975f9c2556bb8e",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4e230798-6d0a-4790-c2c6-016d5d1e9b82"
   },
   "outputs": [],
   "source": [
    "def embed_query(query: str) -> np.ndarray:\n",
    "    q = \"query: \" + query\n",
    "    vec = model.encode(\n",
    "        [q],\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True\n",
    "    ).astype(\"float32\")\n",
    "    return vec\n",
    "\n",
    "query_text = \"Battery measuring\"\n",
    "\n",
    "query_vec = embed_query(query_text)\n",
    "query_vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fdbebcf1cb69b3",
   "metadata": {
    "id": "d8fdbebcf1cb69b3"
   },
   "source": [
    "## Parte 2: FAISS\n",
    "\n",
    "FAISS es una librer\u00eda para b\u00fasqueda por similitud eficiente y clustering de vectores densos.\n",
    "\n",
    "La documentaci\u00f3n de FAISS est\u00e1 disponible en este [link](https://faiss.ai/index.html)\n",
    "\n",
    "### Actividad\n",
    "\n",
    "1. Crea un \u00edndice en FAISS\n",
    "2. Carga los embeddings\n",
    "3. Realiza una b\u00fasqueda a partir de una _query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5097e7479312e742",
   "metadata": {
    "id": "5097e7479312e742",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "22ccb25f-0c81-4b55-fd88-cf186cbd4356"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "import faiss\n",
    "\n",
    "# Dimensi\u00f3n de los embeddings\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "# Crear \u00edndice FAISS\n",
    "index = faiss.IndexFlatIP(dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022acS2AcghG",
   "metadata": {
    "id": "022acS2AcghG",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "87ddd76c-bba4-44cf-f8e2-5c960187d5f1"
   },
   "outputs": [],
   "source": [
    "# Agregar los embeddings al \u00edndice\n",
    "index.add(embeddings)\n",
    "\n",
    "# Verificar cantidad de vectores indexados\n",
    "print(\"Vectores indexados:\", index.ntotal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7kYmO4FWckLc",
   "metadata": {
    "id": "7kYmO4FWckLc",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1a3f1622-69e7-4bca-b7dc-a12aae999c78"
   },
   "outputs": [],
   "source": [
    "# N\u00famero de resultados a recuperar\n",
    "k = 5\n",
    "\n",
    "# B\u00fasqueda en el \u00edndice\n",
    "distances, indices = index.search(query_vec, k)\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    # Add a check to ensure idx is within valid bounds\n",
    "    if 0 <= idx < len(passages):\n",
    "        print(f\"\\nResultado {i + 1}\")\n",
    "        print(f\"Score (similitud): {distances[0][i]:.4f}\")\n",
    "        print(f\"Texto:\\n{passages[idx][:400]}...\")\n",
    "    else:\n",
    "        print(f\"\\nResultado {i + 1} (\u00cdndice inv\u00e1lido: {idx})\")\n",
    "        print(f\"Score (similitud): {distances[0][i]:.4f}\")\n",
    "        print(\"Texto: <No disponible debido a \u00edndice fuera de rango>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f15b24eea20878",
   "metadata": {
    "id": "52f15b24eea20878"
   },
   "source": [
    "## Parte 3 \u2014 Vector DB #1: Qdrant (b\u00fasqueda vectorial + metadata)\n",
    "\n",
    "### Objetivo\n",
    "Recrear el mismo flujo que con FAISS, pero usando una base vectorial con soporte nativo de **metadata** y filtros.\n",
    "\n",
    "### Qu\u00e9 debes implementar\n",
    "1. Levantar / conectar con una instancia de Qdrant.\n",
    "2. Crear una colecci\u00f3n con:\n",
    "   - dimensi\u00f3n `D` (la de tus embeddings)\n",
    "   - m\u00e9trica (cosine o L2)\n",
    "3. Insertar:\n",
    "   - `id`\n",
    "   - `embedding`\n",
    "   - `payload` (metadata: texto, t\u00edtulo, etiquetas, etc.)\n",
    "4. Consultar Top-k por similitud:\n",
    "   - `query_embedding`\n",
    "   - `k`\n",
    "\n",
    "### Inputs esperados (ya definidos arriba en el notebook)\n",
    "- `embeddings`: matriz `N x D` (float32)\n",
    "- `texts`: lista de `N` strings\n",
    "- `metadatas`: lista de `N` dicts (opcional)\n",
    "- `query_text`: string\n",
    "- `query_embedding`: vector `1 x D`\n",
    "\n",
    "### Entregable\n",
    "- Una funci\u00f3n `qdrant_search(query_embedding, k)` que retorne:\n",
    "  - lista de `(id, score, text, metadata)`\n",
    "- Un ejemplo de consulta con `k=5` y su salida.\n",
    "\n",
    "### Preguntas\n",
    "**- \u00bfLa m\u00e9trica usada fue cosine o L2? \u00bfPor qu\u00e9?**\n",
    "Se utilizo la metrica cosine ya que este permite medir que tan alineados estan dos vetores sin verse afectada por su magnitud. Esto es ideal para la busqueda semantica\n",
    "\n",
    "**- \u00bfQu\u00e9 tan f\u00e1cil fue filtrar por metadata en comparaci\u00f3n con FAISS?**\n",
    "Fue mas facil, ya que en FAISS no existen filtros nativos, por lo que filtrar metadata requiere codigo adicional y estructuras externas.\n",
    "En Qdrant el filtrado por metadata es nativo y mas sencillo, ya que cada vector tiene un payload JSON y el motor permite consultas filtradas directamente\n",
    "\n",
    "**- \u00bfQu\u00e9 pasa con el tiempo de respuesta cuando aumentas `k`?**\n",
    "Al aumentar k, el tiempo de respuesta tambien aumenta ligeramente, porque el motor debe devolver y ordenar mas resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2a78ffffb7836",
   "metadata": {
    "id": "9ff2a78ffffb7836",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "1cb9e84f-1723-4dc0-d82d-936e03d6b5db"
   },
   "outputs": [],
   "source": [
    "!pip install -q qdrant-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VHIaKpOXiBEu",
   "metadata": {
    "id": "VHIaKpOXiBEu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0Yq-QmOodxJ8",
   "metadata": {
    "id": "0Yq-QmOodxJ8"
   },
   "outputs": [],
   "source": [
    "qdrant_client = QdrantClient(\":memory:\")  # Correcto para Colab\n",
    "collection_name = \"documents\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aCuht6PseUo9",
   "metadata": {
    "id": "aCuht6PseUo9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d0ffbdc9-6904-4ba1-a46c-88cb860d8e5e"
   },
   "outputs": [],
   "source": [
    "dim = embeddings.shape[1]\n",
    "\n",
    "# Borrar colecci\u00f3n si ya existe (evita errores)\n",
    "try:\n",
    "    qdrant_client.delete_collection(collection_name)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "qdrant_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=dim,\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0LNdcFJeeX_k",
   "metadata": {
    "id": "0LNdcFJeeX_k",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a7183644-7982-46c4-f7e4-b7e2595a140f"
   },
   "outputs": [],
   "source": [
    "points = []\n",
    "\n",
    "for i, emb in enumerate(embeddings):\n",
    "    payload = {\n",
    "        \"text\": passages[i]\n",
    "    }\n",
    "\n",
    "    if 'metadatas' in globals() and metadatas is not None:\n",
    "        payload.update(metadatas[i])\n",
    "\n",
    "    points.append(\n",
    "        PointStruct(\n",
    "            id=i,\n",
    "            vector=emb.tolist(),   #  vector 1D\n",
    "            payload=payload\n",
    "        )\n",
    "    )\n",
    "\n",
    "qdrant_client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5z2xGoaNfNBE",
   "metadata": {
    "id": "5z2xGoaNfNBE"
   },
   "outputs": [],
   "source": [
    "def qdrant_search(query_embedding: np.ndarray, k: int = 5):\n",
    "    \"\"\"\n",
    "    Retorna: (id, score, text, metadata)\n",
    "    Compatible con Qdrant en Google Colab\n",
    "    \"\"\"\n",
    "\n",
    "    # Asegurar vector 1D\n",
    "    if query_embedding.ndim == 2:\n",
    "        query_embedding = query_embedding[0]\n",
    "\n",
    "    search_result = qdrant_client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        with_payload=True\n",
    "    )\n",
    "\n",
    "    results = []\n",
    "    for hit in search_result.points:\n",
    "        results.append((\n",
    "            hit.id,\n",
    "            hit.score,\n",
    "            hit.payload.get(\"text\"),\n",
    "            hit.payload\n",
    "        ))\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IBCPFPvyfPhS",
   "metadata": {
    "id": "IBCPFPvyfPhS",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eeacb225-273e-45a2-d43f-dfca7524db36"
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "results = qdrant_search(query_vec, k)\n",
    "\n",
    "for i, (doc_id, score, text, metadata) in enumerate(results):\n",
    "    print(f\"\\nResultado {i + 1}\")\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Score (similitud): {score:.4f}\")\n",
    "    print(f\"Texto:\\n{text[:400]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bec6e05b842dff",
   "metadata": {
    "id": "69bec6e05b842dff"
   },
   "source": [
    "## Parte 4 \u2014 Vector DB #2: Milvus (indexaci\u00f3n ANN y escalabilidad)\n",
    "\n",
    "### Objetivo\n",
    "Implementar el flujo de indexaci\u00f3n + b\u00fasqueda con una base vectorial orientada a escalabilidad.\n",
    "\n",
    "### Qu\u00e9 debes implementar\n",
    "1. Conectar a Milvus.\n",
    "2. Crear un esquema (colecci\u00f3n) con:\n",
    "   - campo `id` (entero o string)\n",
    "   - campo `embedding` (vector `D`)\n",
    "   - campos de metadata (p.ej., `category`, `source`, `title`)\n",
    "3. Insertar `N` embeddings.\n",
    "4. Crear/seleccionar un \u00edndice ANN (ej. HNSW o IVF).\n",
    "5. Ejecutar consultas Top-k y recuperar textos asociados.\n",
    "\n",
    "### Recomendaci\u00f3n did\u00e1ctica\n",
    "Haz dos configuraciones:\n",
    "- **B\u00fasqueda exacta** (si aplica) o configuraci\u00f3n \u201cm\u00e1s precisa\u201d\n",
    "- **B\u00fasqueda ANN** (configuraci\u00f3n \u201cm\u00e1s r\u00e1pida\u201d)\n",
    "\n",
    "Luego compara:\n",
    "- tiempo de consulta\n",
    "- overlap de resultados (cu\u00e1ntos IDs coinciden)\n",
    "\n",
    "### Entregable\n",
    "- Funci\u00f3n `milvus_search(query_embedding, k)` que devuelva resultados.\n",
    "- Un mini experimento: `k=5` y `k=20` (tiempos y resultados).\n",
    "\n",
    "### Preguntas\n",
    "**- \u00bfQu\u00e9 par\u00e1metros del \u00edndice/control de b\u00fasqueda ajustaste para precisi\u00f3n vs velocidad?**\n",
    "Se ajustaron dos parametros, primero el tipo de indice \"IVF_FLAT\" con \"nlist=128\", que habilita la busqueda aproximada(ANN)\n",
    "El segundo parametro fue el de busqueda \"nprobe\" a un valor de 1 paera que la consulta se realice a un solo cluster haciendolo mas rapido.\n",
    "\n",
    "**- \u00bfQu\u00e9 evidencia tienes de que ANN cambia los resultados (aunque sea poco)?**\n",
    "En el codigo se calcula el porcentaje de coincidencia \"overlap\" entre busquedas. Como el overlap es menor al 100%, se demuestra que ANN devuelve resultados diferentes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uN8dQMVr3wEz",
   "metadata": {
    "id": "uN8dQMVr3wEz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c67ca0f3-d029-4580-cdfe-b399df133027"
   },
   "outputs": [],
   "source": [
    "!pip install -q \"pymilvus[milvus_lite]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sV5PTVv34E4n",
   "metadata": {
    "id": "sV5PTVv34E4n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "62e592b6-66f8-44db-d35a-74a47513689f"
   },
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient, DataType\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# URI SIMPLE, SIN VARIABLES RARAS\n",
    "client = MilvusClient(uri=\"milvus_demo.db\")\n",
    "\n",
    "print(\"Milvus Lite iniciado correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FITE9nwN31QT",
   "metadata": {
    "id": "FITE9nwN31QT"
   },
   "outputs": [],
   "source": [
    "collection_name = \"documents_milvus\"\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "if client.has_collection(collection_name):\n",
    "    client.drop_collection(collection_name)\n",
    "\n",
    "schema = MilvusClient.create_schema(\n",
    "    auto_id=False,\n",
    "    enable_dynamic_field=True\n",
    ")\n",
    "\n",
    "schema.add_field(\"id\", DataType.INT64, is_primary=True)\n",
    "schema.add_field(\"embedding\", DataType.FLOAT_VECTOR, dim=dim)\n",
    "schema.add_field(\"text\", DataType.VARCHAR, max_length=2048)\n",
    "schema.add_field(\"category\", DataType.VARCHAR, max_length=50)\n",
    "\n",
    "index_params = client.prepare_index_params()\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\",\n",
    "    index_type=\"IVF_FLAT\",\n",
    "    metric_type=\"COSINE\",\n",
    "    params={\"nlist\": 128}\n",
    ")\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    index_params=index_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9k6eKxJ46jA",
   "metadata": {
    "id": "e9k6eKxJ46jA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d31724d1-47e5-481b-fd96-cd75c34fcef7"
   },
   "outputs": [],
   "source": [
    "batch_size = 500  # seguro para Colab\n",
    "\n",
    "for start in range(0, len(embeddings), batch_size):\n",
    "    end = start + batch_size\n",
    "\n",
    "    batch_data = [\n",
    "        {\n",
    "            \"id\": i,\n",
    "            \"embedding\": embeddings[i].tolist(),\n",
    "            \"text\": passages[i],\n",
    "            \"category\": \"documento\"\n",
    "        }\n",
    "        for i in range(start, min(end, len(embeddings)))\n",
    "    ]\n",
    "\n",
    "    client.insert(\n",
    "        collection_name=collection_name,\n",
    "        data=batch_data\n",
    "    )\n",
    "\n",
    "    print(f\"Insertados documentos {start} \u2192 {end}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y67qu24548Nf",
   "metadata": {
    "id": "y67qu24548Nf"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def milvus_search(query_embedding, k, nprobe):\n",
    "    \"\"\"\n",
    "    B\u00fasqueda en Milvus Lite\n",
    "    \"\"\"\n",
    "    search_params = {\n",
    "        \"metric_type\": \"COSINE\",\n",
    "        \"params\": {\"nprobe\": nprobe}\n",
    "    }\n",
    "\n",
    "    start = time.time()\n",
    "    results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        data=[query_embedding.tolist()],\n",
    "        limit=k,\n",
    "        search_params=search_params,\n",
    "        output_fields=[\"text\", \"category\"]\n",
    "    )\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "\n",
    "    hits = []\n",
    "    for hit in results[0]:\n",
    "        hits.append((\n",
    "            hit[\"id\"],\n",
    "            hit[\"distance\"],\n",
    "            hit[\"entity\"][\"text\"],\n",
    "            hit[\"entity\"]\n",
    "        ))\n",
    "\n",
    "    return hits, elapsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qABaLjjP482Q",
   "metadata": {
    "id": "qABaLjjP482Q",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "726a45cb-e9c5-46c3-dab9-de7bb8422e51"
   },
   "outputs": [],
   "source": [
    "query_vec = query_vec.squeeze()\n",
    "\n",
    "for k in [5, 20]:\n",
    "    print(f\"\\n\ud83d\udd39 k = {k}\")\n",
    "\n",
    "    # M\u00e1s precisa\n",
    "    res_precise, t_precise = milvus_search(query_vec, k, nprobe=128)\n",
    "\n",
    "    # M\u00e1s r\u00e1pida (ANN)\n",
    "    res_fast, t_fast = milvus_search(query_vec, k, nprobe=1)\n",
    "\n",
    "    ids_precise = {r[0] for r in res_precise}\n",
    "    ids_fast = {r[0] for r in res_fast}\n",
    "\n",
    "    overlap = len(ids_precise & ids_fast) / k * 100\n",
    "\n",
    "    print(f\"Precisa: {t_precise:.2f} ms\")\n",
    "    print(f\"R\u00e1pida:  {t_fast:.2f} ms\")\n",
    "    print(f\"Overlap: {overlap:.1f}%\")\n",
    "    print(f\"Ejemplo texto: {res_fast[0][2][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749c7459cd31499f",
   "metadata": {
    "id": "749c7459cd31499f"
   },
   "source": [
    "## Parte 5 \u2014 Vector DB #3: Weaviate (b\u00fasqueda sem\u00e1ntica con esquema)\n",
    "\n",
    "### Objetivo\n",
    "Montar una colecci\u00f3n con esquema (clase) y ejecutar b\u00fasquedas sem\u00e1nticas Top-k, opcionalmente con filtros.\n",
    "\n",
    "### Qu\u00e9 debes implementar\n",
    "1. Conectar a Weaviate.\n",
    "2. Definir un esquema:\n",
    "   - Clase/colecci\u00f3n (por ejemplo `Document`)\n",
    "   - Propiedades: `text`, `title`, `category`, etc.\n",
    "   - Vector asociado (embedding)\n",
    "3. Insertar objetos con:\n",
    "   - propiedades + vector\n",
    "4. Consultar por similitud (Top-k) con `query_embedding`.\n",
    "5. (Opcional) agregar un filtro por propiedad (metadata).\n",
    "\n",
    "### Recomendaci\u00f3n\n",
    "Aseg\u00farate de guardar el `text` original y al menos 1 campo de metadata para probar filtrado.\n",
    "\n",
    "### Entregable\n",
    "- Funci\u00f3n `weaviate_search(query_embedding, k)` que retorne:\n",
    "  - id, score, text, metadata\n",
    "\n",
    "### Preguntas\n",
    "**- \u00bfQu\u00e9 diferencia conceptual encuentras entre \u201cschema + objetos\u201d vs \u201ctabla + filas\u201d?**\n",
    "El modelo \"schema + objetos\" permite que cada objeto almacene tanto sus datos como su embedding y metadata, integrando directamente la busqueda semantica.\n",
    "El modelo \"tabla + filas\" de una base relacional solo maneja datos estructurados sin representacion semantica nativa, por lo que la busqueda se limita a coincidencias exactas.\n",
    "\n",
    "**- \u00bfC\u00f3mo describir\u00edas el trade-off de complejidad vs expresividad?**\n",
    "\n",
    "Weaviate es mas expresivo porque soporta IA y busqueda vetorial de forma nativa, pero esto tambien lo hace mas complejo de dise\u00f1ar y operar. Las bases relacionales son mas simples y conocidas, aunque menos adecuadas para tareas semanticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25195a86a0105c1d",
   "metadata": {
    "id": "25195a86a0105c1d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "5bb62efa-4dc0-4fec-ef6d-495bdfc5f60d"
   },
   "outputs": [],
   "source": [
    "!pip install weaviate-client\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import time\n"
   ],
   "metadata": {
    "id": "r-6fqWwILZGB"
   },
   "id": "r-6fqWwILZGB",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import weaviate\n",
    "from weaviate.embedded import EmbeddedOptions\n",
    "import time\n",
    "\n",
    "# The embedded_options argument is not directly used in connect_to_embedded() in this client version.\n",
    "# Calling it without arguments will start an embedded instance with default settings.\n",
    "client = weaviate.connect_to_embedded()\n",
    "\n",
    "print(\"Weaviate iniciado correctamente\")"
   ],
   "metadata": {
    "id": "k2aKgyZGLcgg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c3a5d5af-5cda-4226-9936-865be2bc9d96"
   },
   "id": "k2aKgyZGLcgg",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "collection_name = \"Document\"\n",
    "\n",
    "# Si existe, eliminarla\n",
    "if client.collections.exists(collection_name):\n",
    "    client.collections.delete(collection_name)\n",
    "\n",
    "# Crear colecci\u00f3n\n",
    "collection = client.collections.create(\n",
    "    name=collection_name,\n",
    "    vectorizer_config=None,  # embeddings externos\n",
    "    properties=[\n",
    "        weaviate.classes.config.Property(\n",
    "            name=\"text\",\n",
    "            data_type=weaviate.classes.config.DataType.TEXT\n",
    "        ),\n",
    "        weaviate.classes.config.Property(\n",
    "            name=\"category\",\n",
    "            data_type=weaviate.classes.config.DataType.TEXT\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Colecci\u00f3n creada correctamente\")\n"
   ],
   "metadata": {
    "id": "bf1g4c_1MAg1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c51a8084-602c-46aa-fbb4-0ac517534b38"
   },
   "id": "bf1g4c_1MAg1",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "batch_size = 200\n",
    "\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for i in range(len(embeddings)):\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"text\": passages[i],\n",
    "                \"category\": \"documento\"\n",
    "            },\n",
    "            vector=embeddings[i].tolist()\n",
    "        )\n",
    "\n",
    "print(\"Documentos insertados en Weaviate\")\n"
   ],
   "metadata": {
    "id": "N3IDVUVnME77",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d4eff773-b14b-4e31-bbed-203188f6dc11"
   },
   "id": "N3IDVUVnME77",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def weaviate_search(query_embedding, k):\n",
    "    start = time.time()\n",
    "\n",
    "    results = collection.query.near_vector(\n",
    "        near_vector=query_embedding.tolist(),\n",
    "        limit=k,\n",
    "        return_metadata=[\"distance\"]\n",
    "    )\n",
    "\n",
    "    elapsed = (time.time() - start) * 1000\n",
    "\n",
    "    hits = []\n",
    "    for obj in results.objects:\n",
    "        hits.append({\n",
    "            \"id\": obj.uuid,\n",
    "            \"score\": 1 - obj.metadata.distance,\n",
    "            \"text\": obj.properties[\"text\"],\n",
    "            \"metadata\": {\n",
    "                \"category\": obj.properties[\"category\"]\n",
    "            }\n",
    "        })\n",
    "\n",
    "    return hits, elapsed\n"
   ],
   "metadata": {
    "id": "BgTxw5_FMJJs"
   },
   "id": "BgTxw5_FMJJs",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "query_embedding = embeddings[0]\n",
    "\n",
    "for k in [5, 20]:\n",
    "    results, time_ms = weaviate_search(query_embedding, k)\n",
    "\n",
    "    print(f\"\\n\ud83d\udd39 k={k}\")\n",
    "    print(f\"Tiempo: {time_ms:.2f} ms\")\n",
    "    print(f\"Ejemplo texto:\\n{results[0]['text'][:150]}...\")\n"
   ],
   "metadata": {
    "id": "TWUa1a2GMNAe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1fcfbd5-a814-4d74-88b5-a027de3da37f"
   },
   "id": "TWUa1a2GMNAe",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "40919fda773f0fbb",
   "metadata": {
    "id": "40919fda773f0fbb"
   },
   "source": [
    "## Parte 6 \u2014 Vector Store #4: Chroma (prototipado r\u00e1pido)\n",
    "\n",
    "### Objetivo\n",
    "Implementar la misma idea de indexaci\u00f3n y b\u00fasqueda sem\u00e1ntica con una herramienta ligera de prototipado.\n",
    "\n",
    "### Qu\u00e9 debes implementar\n",
    "1. Crear una colecci\u00f3n.\n",
    "2. Insertar:\n",
    "   - ids\n",
    "   - embeddings\n",
    "   - documents (texto)\n",
    "   - metadatas (opcional)\n",
    "3. Consultar Top-k con `query_embedding`.\n",
    "\n",
    "### Nota did\u00e1ctica\n",
    "Chroma es \u00fatil para prototipos: enf\u00f3cate en reproducir el pipeline sin \u201cinfra pesada\u201d.\n",
    "\n",
    "### Entregable\n",
    "- Funci\u00f3n `chroma_search(query_embedding, k)` que retorne resultados.\n",
    "- Una consulta con `k=5`.\n",
    "\n",
    "### Preguntas\n",
    "**- \u00bfQu\u00e9 tan f\u00e1cil fue implementar todo comparado con Qdrant/Milvus?**\n",
    "ChromaDB fue mas facil de implementar que Qdrant y Milvus, porque no requiere definir esquemas, indices ni parametros avanzados. Solo se crea la coleccion y se insertan los embeddings directamente.\n",
    "\n",
    "**- \u00bfQu\u00e9 limitaciones ves para un sistema en producci\u00f3n?**\n",
    "\n",
    "ChromaDB esta mas orientado a prototipos locales. Tiene menos soporte para escalabilidad, alta disponibilidad, control avanzado de indices y rendimiento en grandes volumenes de datos, por lo que no es tan robusto como Qdrant o Milvus para sistemas de produccion de gran escala."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb94e7d911247b",
   "metadata": {
    "id": "52eb94e7d911247b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8dc042d5-4e74-447c-e430-68c169d2454b"
   },
   "outputs": [],
   "source": [
    "!pip install -q chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import time\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"documents\"\n",
    ")"
   ],
   "metadata": {
    "id": "OlI4yHUANDuG"
   },
   "id": "OlI4yHUANDuG",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define ids, documents, and metadatas from previously generated data\n",
    "ids = [str(i) for i in range(len(embeddings))]\n",
    "documents = passages  # Using 'passages' which contains the chunked texts\n",
    "# Create metadatas from chunks_df, ensuring alignment with embeddings/passages\n",
    "metadatas = chunks_df[['doc_id', 'chunk_id']].to_dict(orient='records')\n",
    "\n",
    "# Split into smaller batches as ChromaDB has a batch size limit\n",
    "batch_size = 5000  # A batch size smaller than 5461\n",
    "\n",
    "for i in range(0, len(ids), batch_size):\n",
    "    batch_ids = ids[i:i + batch_size]\n",
    "    batch_documents = documents[i:i + batch_size]\n",
    "    batch_embeddings = embeddings[i:i + batch_size]\n",
    "    batch_metadatas = metadatas[i:i + batch_size]\n",
    "\n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        documents=batch_documents,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadatas\n",
    "    )\n",
    "    print(f\"Inserted batch {i//batch_size + 1} of {len(ids)//batch_size + 1}\")\n",
    "\n"
   ],
   "metadata": {
    "id": "feCsnRw3NtYm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "052b748e-9908-48fe-c93a-993b044554d2"
   },
   "id": "feCsnRw3NtYm",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def chroma_search(query_embedding, k=5):\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "    )\n",
    "\n",
    "    output = []\n",
    "    for i in range(len(results[\"ids\"][0])):\n",
    "        output.append({\n",
    "            \"id\": results[\"ids\"][0][i],\n",
    "            \"score\": results[\"distances\"][0][i],\n",
    "            \"text\": results[\"documents\"][0][i],\n",
    "            \"metadata\": results[\"metadatas\"][0][i] if results[\"metadatas\"] else None\n",
    "        })\n",
    "\n",
    "    return output\n"
   ],
   "metadata": {
    "id": "ISqCoLd8NHYX"
   },
   "id": "ISqCoLd8NHYX",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "results = chroma_search(query_embedding, k=5)\n",
    "\n",
    "for r in results:\n",
    "    print(r)\n"
   ],
   "metadata": {
    "id": "yX7tcQwMOGkr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4ced0165-8015-44e6-b64b-9850c625e718"
   },
   "id": "yX7tcQwMOGkr",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f7b383f0d5f720d",
   "metadata": {
    "id": "f7b383f0d5f720d"
   },
   "source": [
    "## Parte 7 \u2014 SQL + vectores: PostgreSQL/pgvector (vector search transparente)\n",
    "\n",
    "### Objetivo\n",
    "Guardar embeddings en una tabla y ejecutar una consulta SQL de similitud.\n",
    "\n",
    "### Qu\u00e9 debes implementar\n",
    "1. Conectar a una base PostgreSQL con `pgvector` habilitado.\n",
    "2. Crear una tabla (ej. `documents`) con:\n",
    "   - `id` (PK)\n",
    "   - `text` (texto)\n",
    "   - `embedding` (vector(D))\n",
    "   - metadata (columnas adicionales)\n",
    "3. Insertar todos los documentos y embeddings.\n",
    "4. Consultar Top-k por similitud, ordenando por distancia.\n",
    "\n",
    "### F\u00f3rmula conceptual (lo que implementa tu SQL)\n",
    "Para una consulta `q`, buscas:\n",
    "$$ argmin_d \\in D \\; \\text{dist}(\\vec{q}, \\vec{d})$$\n",
    "donde `dist` puede ser L2 o una variante para cosine (seg\u00fan configuraci\u00f3n).\n",
    "\n",
    "### Entregable\n",
    "- Funci\u00f3n `pgvector_search(query_embedding, k)` que ejecute SQL y devuelva:\n",
    "  - id, score/distancia, text, metadata\n",
    "\n",
    "### Preguntas\n",
    "**- \u00bfQu\u00e9 tan \u201cexplicable\u201d te parece esta aproximaci\u00f3n vs las otras?**\n",
    "SQL es mas explicable y transparente porque ves claramente como se calcula la similitud y como se ordenan los resultados.\n",
    "\n",
    "**- \u00bfQu\u00e9 ventajas ofrece el mundo SQL (JOIN, filtros, agregaciones)?**\n",
    "SQL permite JOINs, filtros complejos y agregaciones nativas, lo que facilita combinar embeddings con datos estructurados. Esto hace que construir analisis y reportes sea muy flexible sin salirte del motor SQL.\n",
    "\n",
    "**- \u00bfQu\u00e9 limitaciones esperas en escalabilidad frente a bases vectoriales dedicadas?**\n",
    "SQL no escala tan bien como Qdrant o Milvus para millones de vetores, porque normalmente usa busqueda exacta y no indices ANN optimizados. Esto aumenta la latencia y el consumo de recursos cuando crece el volumen de datos o las consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b4b4b96bb8b8bb",
   "metadata": {
    "id": "c3b4b4b96bb8b8bb",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e81cb92-9d0d-46be-b912-532dfc365c67"
   },
   "outputs": [],
   "source": [
    "pip install psycopg2-binary numpy"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. Configuraci\u00f3n\n",
    "# --------------------------------------------------\n",
    "DIMENSION = len(embeddings[0])\n",
    "con = duckdb.connect(\":memory:\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. Crear tabla\n",
    "# --------------------------------------------------\n",
    "con.sql(f\"\"\"\n",
    "CREATE TABLE documents (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    text VARCHAR,\n",
    "    embedding FLOAT[{DIMENSION}],\n",
    "    metadata JSON\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. Insertar documentos + embeddings\n",
    "# --------------------------------------------------\n",
    "data_to_insert = []\n",
    "\n",
    "for i in range(len(documents)):\n",
    "    vec = embeddings[i].tolist()\n",
    "    meta = json.dumps(metadatas[i]) if isinstance(metadatas[i], dict) else json.dumps({})\n",
    "    txt = documents[i]\n",
    "    data_to_insert.append((i, txt, vec, meta))\n",
    "\n",
    "con.executemany(\n",
    "    \"INSERT INTO documents VALUES (?, ?, ?, ?)\",\n",
    "    data_to_insert\n",
    ")\n",
    "\n",
    "print(f\"Documentos insertados: {len(data_to_insert)}\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. Funci\u00f3n entregable: pgvector_search\n",
    "# --------------------------------------------------\n",
    "def pgvector_search(query_embedding, k=5):\n",
    "    \"\"\"\n",
    "    Ejecuta b\u00fasqueda sem\u00e1ntica Top-k usando SQL + vectores.\n",
    "    Retorna: id, score/distancia, text, metadata\n",
    "    \"\"\"\n",
    "    q_list = query_embedding.tolist() if hasattr(query_embedding, \"tolist\") else query_embedding\n",
    "\n",
    "    results = con.sql(f\"\"\"\n",
    "        SELECT\n",
    "            id,\n",
    "            (1.0 - list_cosine_similarity(embedding, {q_list}::FLOAT[{DIMENSION}])) AS distance,\n",
    "            text,\n",
    "            metadata\n",
    "        FROM documents\n",
    "        ORDER BY distance ASC\n",
    "        LIMIT {k}\n",
    "    \"\"\").fetchall()\n",
    "\n",
    "    response = []\n",
    "    for r in results:\n",
    "        response.append({\n",
    "            \"id\": r[0],\n",
    "            \"score\": r[1],          # distancia (menor = m\u00e1s similar)\n",
    "            \"text\": r[2],\n",
    "            \"metadata\": json.loads(r[3])\n",
    "        })\n",
    "\n",
    "    return response\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5. Consulta de prueba\n",
    "# --------------------------------------------------\n",
    "query_embedding = embeddings[0]  # o uno nuevo generado\n",
    "results = pgvector_search(query_embedding, k=5)\n",
    "\n",
    "for r in results:\n",
    "    print(r)\n"
   ],
   "metadata": {
    "id": "poJpy-7fPLtp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8b850f58-c586-4304-ae9b-48f1edf64de1"
   },
   "id": "poJpy-7fPLtp",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import output\n",
    "output.clear()"
   ],
   "metadata": {
    "id": "1d5zyAyWfEKO"
   },
   "id": "1d5zyAyWfEKO",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}